{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df = pd.read_csv(\"../input/ss3-hackathon-online-signal-processing/SN_Test/test_data_1.csv\")\ndf = pd.read_csv(\"../input/ss3-hackathon-online-signal-processing/SN_Train/normal_1411_1.csv\")\n# df = pd.read_csv(\"../input/ss3-hackathon-online-signal-processing/SN_Train/normal_576_1.csv\")\n# df = pd.read_csv(\"../input/ss3-hackathon-online-signal-processing/SN_Train/normal_576_10.csv\")\n# df = pd.read_csv(\"../input/ss3-hackathon-online-signal-processing/SN_Train/unbalanceM1_1411_1.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()\n# df.columns\n# df.info()\n# df['V1A'] = pd.to_numeric(df['V1A'], errors='coerce')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Read file\nfunctional, clean data, remove DC Bias, reduce noise by lfilter, transpose_to( #files, #sample_size, #components )","metadata":{}},{"cell_type":"code","source":"from scipy.signal import lfilter\n\nTRAIN_FOLDER = '/kaggle/input/ss3-hackathon-online-signal-processing/SN_Train/'\nTEST_FOLDER = '/kaggle/input/ss3-hackathon-online-signal-processing/SN_Test/'\n\ndef do_lfilter(y):\n    n = 15  # the larger n is, the smoother curve will be\n    b = [1.0 / n] * n\n    a = 1\n    yy = lfilter(b, a, y)\n    return yy\n\ndef folder_tosignal(folder):\n    md = 1e9\n    fi2 = True\n    for f in os.listdir(folder):\n        df = pd.read_csv(folder + f)\n        df = df[df['waveform'].str.contains('t0')==False]\n        df = df[df['waveform'].str.contains('delta t')==False]\n        df = df[df['waveform'].str.contains('time')==False]\n        df = df[df['waveform'].str.contains('wave')==False]\n        df = df.drop(['waveform'], axis=1)\n        fi = True\n        for col in df.columns:\n            df[col] = pd.to_numeric(df[col])\n            signal = df[col].to_numpy()\n            signal = signal - np.mean(signal) # Remove DC Bias\n            signal = do_lfilter(signal) # Reduce Noise by lfilter\n            signal_components = signal if fi else np.vstack((signal_components, signal)) # stack signal (10000+ sample) each sensor component (6)\n            fi = False\n        md = min(signal_components.shape[1], md) # select least sample size for all signals\n        all_signals = signal_components if fi2 else np.dstack((all_signals[:, :md], signal_components[:, :md])) # stack motor signal (1000+)\n        print(f'converting {f}: {all_signals.shape}')\n        fi2 = False\n\n    all_signals = np.transpose(all_signals, (2, 1, 0))\n    return all_signals\n\ntrain_signal = folder_tosignal(TRAIN_FOLDER)\ntest_signal = folder_tosignal(TEST_FOLDER)","metadata":{"execution":{"iopub.status.busy":"2022-11-12T14:17:57.530115Z","iopub.execute_input":"2022-11-12T14:17:57.530623Z","iopub.status.idle":"2022-11-12T14:26:31.843599Z","shell.execute_reply.started":"2022-11-12T14:17:57.530584Z","shell.execute_reply":"2022-11-12T14:26:31.841384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_file_name(folder):\n    l = []\n    for f in os.listdir(folder):\n        l.append(f)\n    return pd.DataFrame(l)\n\nfile_df = get_file_name(TEST_FOLDER)\nfile_df","metadata":{"execution":{"iopub.status.busy":"2022-11-12T14:26:31.847946Z","iopub.execute_input":"2022-11-12T14:26:31.848394Z","iopub.status.idle":"2022-11-12T14:26:31.886212Z","shell.execute_reply.started":"2022-11-12T14:26:31.848349Z","shell.execute_reply":"2022-11-12T14:26:31.884906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_file_num(folder):\n    l = []\n    for f in os.listdir(folder):\n        l.append(f.split('_')[2].split('.')[0])\n    return pd.DataFrame(l).astype('int64')\n\nnum_df = get_file_num(TEST_FOLDER)\nnum_df","metadata":{"execution":{"iopub.status.busy":"2022-11-12T14:26:31.888049Z","iopub.execute_input":"2022-11-12T14:26:31.889436Z","iopub.status.idle":"2022-11-12T14:26:31.907844Z","shell.execute_reply.started":"2022-11-12T14:26:31.889386Z","shell.execute_reply":"2022-11-12T14:26:31.906393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train เก่า\nTRAIN_FOLDER = '/kaggle/input/ss3-hackathon-online-signal-processing/SN_Train/'\n\ntest_file = ['normal_1411_1.csv', 'unbalanceM1_1411_1.csv', 'unbalanceM2_1411_1.csv']\n\nx_train = np.array([])\ny_train = []\nmd = 1e9\nfi2 = True\n\nfor f in os.listdir(TRAIN_FOLDER):\n    df = pd.read_csv(TRAIN_FOLDER + f)\n    df = df[df['waveform'].str.contains('t0')==False]\n    df = df[df['waveform'].str.contains('delta t')==False]\n    df = df[df['waveform'].str.contains('time')==False]\n    df = df[df['waveform'].str.contains('wave')==False]\n    df = df.drop(['waveform'], axis=1)\n    fi = True\n    for col in df.columns:\n        df[col] = pd.to_numeric(df[col])\n        sig = df[col].to_numpy()\n#         print(sig.shape)\n        s = sig if fi else np.vstack((s, sig))\n#         print(s.shape)\n        fi = False\n    c = f.split('_')[0]\n    y_train.append(c)\n#     print(s.shape, x_train.shape)\n    md = min(s.shape[1], md)\n#     print(md)\n    x_train = s if fi2 else np.dstack((x_train[:, :md], s[:, :md]))\n    print(f'converting {f}: {x_train.shape}')\n    fi2 = False\n#     x_train.append(s)\n\nt = np.transpose(x_train, (2, 1, 0))","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test\nTEST_FOLDER = '/kaggle/input/ss3-hackathon-online-signal-processing/SN_Test/'\n\nx_test = np.array([])\nmd = 1e9\nfi2 = True\n\nfor f in os.listdir(TEST_FOLDER):\n    df = pd.read_csv(TEST_FOLDER + f)\n    df = df[df['waveform'].str.contains('t0')==False]\n    df = df[df['waveform'].str.contains('delta t')==False]\n    df = df[df['waveform'].str.contains('time')==False]\n    df = df[df['waveform'].str.contains('wave')==False]\n    df = df.drop(['waveform'], axis=1)\n    fi = True\n    for col in df.columns:\n        df[col] = pd.to_numeric(df[col])\n        sig = df[col].to_numpy()\n#         print(sig.shape)\n        s = sig if fi else np.vstack((s, sig))\n#         print(s.shape)\n        fi = False\n#     c = f.split('_')[0]\n#     y_train.append(c)\n#     print(s.shape, x_train.shape)\n    md = min(s.shape[1], md)\n#     print(md)\n    x_test = s if fi2 else np.dstack((x_test[:, :md], s[:, :md]))\n    print(f'converting {f}: {x_test.shape}')\n    fi2 = False\n#     x_train.append(s)\n\nX_test = np.transpose(x_test, (2, 1, 0))","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test.shape","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"t = np.transpose(x_train, (2, 1, 0))\ntt = np.transpose(x_train, (1, 2, 0))","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"N = train_signals.shape[1]\nf_s = 10000\nT = 1 / f_s","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"N, f_s, T","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# the number of signals in the training set is 7352 \n# and the number of signals in the test set is 2947 \n# And each signal in the training and test set has a length of 128 samples \n# and 9 different components.\n# (7352, 128, 9)\n# (2947, 128, 9)\n# My Signal\n# (1000, 10000, 6) use t (2, 1, 0)\nx_train.shape, t.shape, tt.shape","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = t\nX_train.shape","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y_train = np.array(y_train)","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfy = pd.DataFrame(y_train)\ndfy.iloc[:, 0].unique()","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Encode Y_train","metadata":{}},{"cell_type":"code","source":"INPUT_FOLDER_TRAIN = '/kaggle/input/ss3-hackathon-online-signal-processing/SN_Train/'\n\ndef read_labels(folder):\n    motors = []\n    for file in os.listdir(folder):\n        m = file.split('_')[0]\n        motors.append(m)\n    return np.array(motors)\n\ntrain_labels = read_labels(INPUT_FOLDER_TRAIN)\ntrain_labels","metadata":{"execution":{"iopub.status.busy":"2022-11-12T14:26:31.911203Z","iopub.execute_input":"2022-11-12T14:26:31.912108Z","iopub.status.idle":"2022-11-12T14:26:31.925387Z","shell.execute_reply.started":"2022-11-12T14:26:31.912060Z","shell.execute_reply":"2022-11-12T14:26:31.924049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import preprocessing\nle = preprocessing.LabelEncoder()\nle.fit(['normal', 'unbalanceM1', 'unbalanceM2', 'unbalanceM3'])\nY_train = le.transform(train_labels)\nY_train","metadata":{"execution":{"iopub.status.busy":"2022-11-12T14:26:31.927258Z","iopub.execute_input":"2022-11-12T14:26:31.927751Z","iopub.status.idle":"2022-11-12T14:26:31.940656Z","shell.execute_reply.started":"2022-11-12T14:26:31.927708Z","shell.execute_reply":"2022-11-12T14:26:31.939541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y_train, Y_train.shape","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.save('X_train.npy', X_train)","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = np.load('X_train.npy')\nx.shape","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.shape, Y_train.shape, X_test.shape","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_signals = X_train\ntrain_labels = Y_train\ntest_signals = X_test","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TRAIN_FOLDER = '/kaggle/input/ss3-hackathon-online-signal-processing/SN_Train'\nTEST_FOLDER = '/kaggle/input/ss3-hackathon-online-signal-processing/SN_Test'\nx_train = np.array([])\nrpm = []\ny_train = []\ntest_file = ['normal_1411_1.csv', 'unbalanceM1_1411_1.csv', 'unbalanceM2_1411_1.csv']\nfi2 = True\nmd = 100000\nfor f in test_file: # os.listdir(TRAIN_FOLDER):\n    df = pd.read_csv(TRAIN_FOLDER + '/' + f)\n    df = df[df['waveform'].str.contains('t0')==False]\n    df = df[df['waveform'].str.contains('delta t')==False]\n    df = df[df['waveform'].str.contains('time')==False]\n    df = df[df['waveform'].str.contains('wave')==False]\n    df = df.drop(['waveform'], axis=1)\n    fi = True\n    for col in df.columns:\n        df[col] = pd.to_numeric(df[col])\n        sig = df[col].to_numpy()\n#         print(sig.shape)\n        s = sig if fi else np.vstack((s, sig))\n#         print(s.shape)\n        fi = False\n    c, r = f.split('_')[:2]\n    rpm.append(r)\n    y_train.append(c)\n    print(s.shape, x_train.shape)\n    md = min(s.shape[1], md)\n#     print(md)\n    x_train = s if fi2 else np.dstack((x_train[:, :md], s[:, :md]))\n    fi2 = False\n#     x_train.append(s)\n\nt = x_train.reshape((3, 19993, 6))\nx_train.shape, t.shape, s.shape, s.transpose().shape","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def read_signals(filename):\n    df = pd.read_csv(filename)\n    df = df[df['waveform'].str.contains('t0')==False]\n    df = df[df['waveform'].str.contains('delta t')==False]\n    df = df[df['waveform'].str.contains('time')==False]\n    df = df[df['waveform'].str.contains('wave')==False]\n    df = df.drop(['waveform'], axis=1)\n    data = np.array(df, dtype=np.float32)\n    print(f'converting {filename}: {data.shape}')\n    return data\n\ndef read_labels(folder):\n    motors = []\n    for file in os.listdir(folder):\n        m = f.split('_')[0]\n    return np.array(motors)\n\n# def read_labels(foldername):        \n#     with open(filename, 'r') as fp:\n#         activities = fp.read().splitlines()\n#         activities = list(map(int, activities))\n#     return np.array(motors)\n\nINPUT_FOLDER_TRAIN = '/kaggle/input/ss3-hackathon-online-signal-processing/SN_Train/'\nINPUT_FOLDER_TEST = '/kaggle/input/ss3-hackathon-online-signal-processing/SN_Train/'\n\ntrain_signals, test_signals = [], []\n\nmd = 1e9\n\nfor input_file in os.listdir(INPUT_FOLDER_TRAIN):\n    signal = read_signals(INPUT_FOLDER_TRAIN + input_file)\n#     md = min(signal[0], md)\n#     train_signals.append(signal[:md, :])\n    train_signals.append(signal)\ntrain_signals = np.transpose(np.array(train_signals), (1, 2, 0))\n\nfor input_file in os.listdir(INPUT_FOLDER_TEST):\n    signal = read_signals(INPUT_FOLDER_TEST + input_file)\n    test_signals.append(signal)\ntest_signals = np.transpose(np.array(test_signals), (1, 2, 0))","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_labels","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_signals","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Plot graph","metadata":{}},{"cell_type":"code","source":"from scipy.fftpack import fft\nfrom scipy.signal import welch\nimport numpy as np\nfrom mpl_toolkits.mplot3d import Axes3D\nimport matplotlib.pyplot as plt","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['V1A'].count()","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%config Completer.use_jedi = False","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lfilter\nfrom scipy.signal import lfilter\n\nn = 15  # the larger n is, the smoother curve will be\nb = [1.0 / n] * n\na = 1\nyy = lfilter(b, a, y)\nplt.plot(x, yy, linewidth=2, linestyle=\"-\", c=\"b\")  # smooth by filter","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots()\nN = df['V1A'].count()\n# N = 1000\nT = 1.0 / 10000.0\nx = np.linspace(0, 10, N)\ny = df.iloc[:N, 2].to_numpy()\ny = y - np.mean(y) # DC Offset !!!\nax.plot(x, y)\n# ax.scatter(x, df.iloc[:N, 2])\nplt.show()\n\nfig, ax = plt.subplots()\nx = np.linspace(0.0, N*T, N, endpoint=False)\nyf = fft(yy)\nxf = fftfreq(N, T)[:N//2]\nplt.plot(xf, 2.0/N * np.abs(yf[0:N//2]))\nplt.grid()\nplt.show()","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_fft_values(y_values, T, N, f_s):\n    f_values = np.linspace(0.0, 1.0/(2.0*T), N//2)\n    fft_values_ = fft(y_values)\n    fft_values = 2.0/N * np.abs(fft_values_[0:N//2])\n    return f_values, fft_values\n \nt_n = 10\nN = df['V1A'].count()\n# N = 1000\nT = t_n / N\n# T = 1 / f_s\n# f_s = 10000\n \nf_values, fft_values = get_fft_values(df.iloc[:N, 2].to_numpy(), T, N, f_s)\n \nplt.plot(f_values, fft_values, linestyle='-', color='blue')\nplt.xlabel('Frequency [Hz]', fontsize=16)\nplt.ylabel('Amplitude', fontsize=16)\nplt.title(\"Frequency domain of the signal\", fontsize=16)\nplt.show()","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f_values, psd_values = get_psd_values(df.iloc[:N, 2].to_numpy(), T, N, f_s)\n \nplt.plot(f_values, psd_values, linestyle='-', color='blue')\nplt.xlabel('Frequency [Hz]')\nplt.ylabel('PSD [V**2 / Hz]')\nplt.show()","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"yf.dtype","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Copy preprocess","metadata":{}},{"cell_type":"code","source":"def get_fft_values(y_values, T, N, f_s):\n    f_values = np.linspace(0.0, 1.0/(2.0*T), N//2)\n    fft_values_ = fft(y_values)\n    fft_values = 2.0/N * np.abs(fft_values_[0:N//2])\n    return f_values, fft_values\n\ndef get_psd_values(y_values, T, N, f_s):\n    f_values, psd_values = welch(y_values, fs=f_s)\n    return f_values, psd_values\n\ndef autocorr(x):\n    result = np.correlate(x, x, mode='full')\n    return result[len(result)//2:]\n\ndef get_autocorr_values(y_values, T, N, f_s):\n    autocorr_values = autocorr(y_values)\n    x_values = np.array([T * jj for jj in range(0, N)])\n    return x_values, autocorr_values","metadata":{"execution":{"iopub.status.busy":"2022-11-12T14:26:31.942062Z","iopub.execute_input":"2022-11-12T14:26:31.942506Z","iopub.status.idle":"2022-11-12T14:26:31.952066Z","shell.execute_reply.started":"2022-11-12T14:26:31.942473Z","shell.execute_reply":"2022-11-12T14:26:31.950716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Copied From http://nbviewer.ipython.org/github/demotu/BMC/blob/master/notebooks/DetectPeaks.ipynb\n# Thank you Marcos Duarte\n\ndef detect_peaks(x, mph=None, mpd=1, threshold=0, edge='rising',\n                 kpsh=False, valley=False, show=False, ax=None):\n\n    \"\"\"Detect peaks in data based on their amplitude and other features.\n    Parameters\n    ----------\n    x : 1D array_like\n        data.\n    mph : {None, number}, optional (default = None)\n        detect peaks that are greater than minimum peak height.\n    mpd : positive integer, optional (default = 1)\n        detect peaks that are at least separated by minimum peak distance (in\n        number of data).\n    threshold : positive number, optional (default = 0)\n        detect peaks (valleys) that are greater (smaller) than `threshold`\n        in relation to their immediate neighbors.\n    edge : {None, 'rising', 'falling', 'both'}, optional (default = 'rising')\n        for a flat peak, keep only the rising edge ('rising'), only the\n        falling edge ('falling'), both edges ('both'), or don't detect a\n        flat peak (None).\n    kpsh : bool, optional (default = False)\n        keep peaks with same height even if they are closer than `mpd`.\n    valley : bool, optional (default = False)\n        if True (1), detect valleys (local minima) instead of peaks.\n    show : bool, optional (default = False)\n        if True (1), plot data in matplotlib figure.\n    ax : a matplotlib.axes.Axes instance, optional (default = None).\n    Returns\n    -------\n    ind : 1D array_like\n        indeces of the peaks in `x`.\n    Notes\n    -----\n    The detection of valleys instead of peaks is performed internally by simply\n    negating the data: `ind_valleys = detect_peaks(-x)`\n    \n    The function can handle NaN's \n    See this IPython Notebook [1]_.\n    References\n    ----------\n    .. [1] http://nbviewer.ipython.org/github/demotu/BMC/blob/master/notebooks/DetectPeaks.ipynb\n    Examples\n    --------\n    >>> from detect_peaks import detect_peaks\n    >>> x = np.random.randn(100)\n    >>> x[60:81] = np.nan\n    >>> # detect all peaks and plot data\n    >>> ind = detect_peaks(x, show=True)\n    >>> print(ind)\n    >>> x = np.sin(2*np.pi*5*np.linspace(0, 1, 200)) + np.random.randn(200)/5\n    >>> # set minimum peak height = 0 and minimum peak distance = 20\n    >>> detect_peaks(x, mph=0, mpd=20, show=True)\n    >>> x = [0, 1, 0, 2, 0, 3, 0, 2, 0, 1, 0]\n    >>> # set minimum peak distance = 2\n    >>> detect_peaks(x, mpd=2, show=True)\n    >>> x = np.sin(2*np.pi*5*np.linspace(0, 1, 200)) + np.random.randn(200)/5\n    >>> # detection of valleys instead of peaks\n    >>> detect_peaks(x, mph=0, mpd=20, valley=True, show=True)\n    >>> x = [0, 1, 1, 0, 1, 1, 0]\n    >>> # detect both edges\n    >>> detect_peaks(x, edge='both', show=True)\n    >>> x = [-2, 1, -2, 2, 1, 1, 3, 0]\n    >>> # set threshold = 2\n    >>> detect_peaks(x, threshold = 2, show=True)\n    \"\"\"\n\n    x = np.atleast_1d(x).astype('float64')\n    if x.size < 3:\n        return np.array([], dtype=int)\n    if valley:\n        x = -x\n    # find indices of all peaks\n    dx = x[1:] - x[:-1]\n    # handle NaN's\n    indnan = np.where(np.isnan(x))[0]\n    if indnan.size:\n        x[indnan] = np.inf\n        dx[np.where(np.isnan(dx))[0]] = np.inf\n    ine, ire, ife = np.array([[], [], []], dtype=int)\n    if not edge:\n        ine = np.where((np.hstack((dx, 0)) < 0) & (np.hstack((0, dx)) > 0))[0]\n    else:\n        if edge.lower() in ['rising', 'both']:\n            ire = np.where((np.hstack((dx, 0)) <= 0) & (np.hstack((0, dx)) > 0))[0]\n        if edge.lower() in ['falling', 'both']:\n            ife = np.where((np.hstack((dx, 0)) < 0) & (np.hstack((0, dx)) >= 0))[0]\n    ind = np.unique(np.hstack((ine, ire, ife)))\n    # handle NaN's\n    if ind.size and indnan.size:\n        # NaN's and values close to NaN's cannot be peaks\n        ind = ind[np.in1d(ind, np.unique(np.hstack((indnan, indnan-1, indnan+1))), invert=True)]\n    # first and last values of x cannot be peaks\n    if ind.size and ind[0] == 0:\n        ind = ind[1:]\n    if ind.size and ind[-1] == x.size-1:\n        ind = ind[:-1]\n    # remove peaks < minimum peak height\n    if ind.size and mph is not None:\n        ind = ind[x[ind] >= mph]\n    # remove peaks - neighbors < threshold\n    if ind.size and threshold > 0:\n        dx = np.min(np.vstack([x[ind]-x[ind-1], x[ind]-x[ind+1]]), axis=0)\n        ind = np.delete(ind, np.where(dx < threshold)[0])\n    # detect small peaks closer than minimum peak distance\n    if ind.size and mpd > 1:\n        ind = ind[np.argsort(x[ind])][::-1]  # sort ind by peak height\n        idel = np.zeros(ind.size, dtype=bool)\n        for i in range(ind.size):\n            if not idel[i]:\n                # keep peaks with the same height if kpsh is True\n                idel = idel | (ind >= ind[i] - mpd) & (ind <= ind[i] + mpd) \\\n                    & (x[ind[i]] > x[ind] if kpsh else True)\n                idel[i] = 0  # Keep current peak\n        # remove the small peaks and sort back the indices by their occurrence\n        ind = np.sort(ind[~idel])\n\n    if show:\n        if indnan.size:\n            x[indnan] = np.nan\n        if valley:\n            x = -x\n        _plot(x, mph, mpd, threshold, edge, valley, ax, ind)\n\n    return ind\n","metadata":{"execution":{"iopub.status.busy":"2022-11-12T14:26:31.953950Z","iopub.execute_input":"2022-11-12T14:26:31.954325Z","iopub.status.idle":"2022-11-12T14:26:31.979756Z","shell.execute_reply.started":"2022-11-12T14:26:31.954293Z","shell.execute_reply":"2022-11-12T14:26:31.978562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_first_n_peaks(x,y,no_peaks=5):\n    x_, y_ = list(x), list(y)\n    if len(x_) >= no_peaks:\n        return x_[:no_peaks], y_[:no_peaks]\n    else:\n        missing_no_peaks = no_peaks-len(x_)\n        return x_ + [0]*missing_no_peaks, y_ + [0]*missing_no_peaks\n    \ndef get_features(x_values, y_values, mph):\n    indices_peaks = detect_peaks(y_values, mph=mph)\n    peaks_x, peaks_y = get_first_n_peaks(x_values[indices_peaks], y_values[indices_peaks])\n    return peaks_x + peaks_y\n\ndef extract_features(dataset, T, N, f_s, denominator):\n    percentile = 5\n    list_of_features = []\n    for signal_no in range(0, len(dataset)):\n        features = []\n        for signal_comp in range(0,dataset.shape[2]):\n            signal = dataset[signal_no, :, signal_comp]\n            \n            signal_min = np.nanpercentile(signal, percentile)\n            signal_max = np.nanpercentile(signal, 100-percentile)\n            #ijk = (100 - 2*percentile)/10\n            mph = signal_min + (signal_max - signal_min)/denominator\n            \n            features += get_features(*get_psd_values(signal, T, N, f_s), mph)\n            features += get_features(*get_fft_values(signal, T, N, f_s), mph)\n            features += get_features(*get_autocorr_values(signal, T, N, f_s), mph)\n        list_of_features.append(features)\n    return np.array(list_of_features)\n\n# def extract_features_labels(dataset, labels, T, N, f_s, denominator):\n#     percentile = 5\n#     list_of_features = []\n#     list_of_labels = []\n#     for signal_no in range(0, len(dataset)):\n#         features = []\n#         list_of_labels.append(labels[signal_no])\n#         for signal_comp in range(0,dataset.shape[2]):\n#             signal = dataset[signal_no, :, signal_comp]\n            \n#             signal_min = np.nanpercentile(signal, percentile)\n#             signal_max = np.nanpercentile(signal, 100-percentile)\n#             #ijk = (100 - 2*percentile)/10\n#             mph = signal_min + (signal_max - signal_min)/denominator\n            \n#             features += get_features(*get_psd_values(signal, T, N, f_s), mph)\n#             features += get_features(*get_fft_values(signal, T, N, f_s), mph)\n#             features += get_features(*get_autocorr_values(signal, T, N, f_s), mph)\n#         list_of_features.append(features)\n#     return np.array(list_of_features), np.array(list_of_labels)\n\nfrom scipy.fftpack import fft\nfrom scipy.signal import welch\nimport numpy as np\nfrom mpl_toolkits.mplot3d import Axes3D\nimport matplotlib.pyplot as plt\n\ndenominator = 10\nN = train_signal.shape[1]\nf_s = 10000\nT = 1 / f_s\nX_train = extract_features(train_signal, T, N, f_s, denominator)\nN = test_signal.shape[1]\nX_test = extract_features(test_signal, T, N, f_s, denominator)\n# X_train = extract_features_labels(train_signals, T, N, f_s, denominator)\n# X_test = extract_features_labels(test_signals, T, N, f_s, denominator)","metadata":{"execution":{"iopub.status.busy":"2022-11-12T14:30:06.335648Z","iopub.execute_input":"2022-11-12T14:30:06.336255Z","iopub.status.idle":"2022-11-12T15:16:36.974750Z","shell.execute_reply.started":"2022-11-12T14:30:06.336190Z","shell.execute_reply":"2022-11-12T15:16:36.971864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.shape, Y_train.shape, X_test.shape","metadata":{"execution":{"iopub.status.busy":"2022-11-12T15:16:36.980411Z","iopub.execute_input":"2022-11-12T15:16:36.984710Z","iopub.status.idle":"2022-11-12T15:16:37.005905Z","shell.execute_reply.started":"2022-11-12T15:16:36.984638Z","shell.execute_reply":"2022-11-12T15:16:37.004492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Improve Model","metadata":{}},{"cell_type":"markdown","source":"### split train-val for model improving","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Random Forest","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report\n \nclf = RandomForestClassifier(n_estimators=1000)\nclf.fit(X_train, Y_train)\nprint(\"Accuracy on training set is : {}\".format(clf.score(X_train, Y_train)))\nY_test_pred = clf.predict(X_test)\nY_test_pred","metadata":{"execution":{"iopub.status.busy":"2022-11-12T16:02:06.266549Z","iopub.execute_input":"2022-11-12T16:02:06.266979Z","iopub.status.idle":"2022-11-12T16:02:12.365498Z","shell.execute_reply.started":"2022-11-12T16:02:06.266943Z","shell.execute_reply":"2022-11-12T16:02:12.364114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -Uqq fastai","metadata":{"execution":{"iopub.status.busy":"2022-11-12T16:37:37.258290Z","iopub.execute_input":"2022-11-12T16:37:37.259581Z","iopub.status.idle":"2022-11-12T16:37:51.318980Z","shell.execute_reply.started":"2022-11-12T16:37:37.259524Z","shell.execute_reply":"2022-11-12T16:37:51.317347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install fastai==2.1.4\n!pip install fastcore==1.3.1","metadata":{"execution":{"iopub.status.busy":"2022-11-12T16:26:56.020174Z","iopub.execute_input":"2022-11-12T16:26:56.020731Z","iopub.status.idle":"2022-11-12T16:27:22.060392Z","shell.execute_reply.started":"2022-11-12T16:26:56.020686Z","shell.execute_reply":"2022-11-12T16:27:22.058732Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### fastai tabular","metadata":{}},{"cell_type":"code","source":"from fastai.tabular.all import *","metadata":{"execution":{"iopub.status.busy":"2022-11-12T16:08:31.663904Z","iopub.execute_input":"2022-11-12T16:08:31.665201Z","iopub.status.idle":"2022-11-12T16:08:31.672190Z","shell.execute_reply.started":"2022-11-12T16:08:31.665140Z","shell.execute_reply":"2022-11-12T16:08:31.670889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame(X_train)\ndf['label'] = pd.DataFrame(Y_train)\n# df['label'] = pd.DataFrame(train_labels)\ndf","metadata":{"execution":{"iopub.status.busy":"2022-11-12T16:53:33.769076Z","iopub.execute_input":"2022-11-12T16:53:33.769808Z","iopub.status.idle":"2022-11-12T16:53:33.824845Z","shell.execute_reply.started":"2022-11-12T16:53:33.769766Z","shell.execute_reply":"2022-11-12T16:53:33.823480Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cont_names = list(range(180))\n# cont_names = [str(i) for i in list(range(180))]\n\n# splits = RandomSplitter()(range_of(df))\nsplits = RandomSplitter(valid_pct=0.2)(range_of(df))\n\nto = TabularPandas(df, procs=[Categorify, FillMissing, Normalize],\n                   cont_names = cont_names,\n                   y_names='label',\n                   y_block = CategoryBlock,\n                   splits=splits)","metadata":{"execution":{"iopub.status.busy":"2022-11-12T16:53:35.567942Z","iopub.execute_input":"2022-11-12T16:53:35.568395Z","iopub.status.idle":"2022-11-12T16:53:35.682647Z","shell.execute_reply.started":"2022-11-12T16:53:35.568356Z","shell.execute_reply":"2022-11-12T16:53:35.681668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"to.xs.iloc[:2]","metadata":{"execution":{"iopub.status.busy":"2022-11-12T16:53:36.096861Z","iopub.execute_input":"2022-11-12T16:53:36.097397Z","iopub.status.idle":"2022-11-12T16:53:36.127810Z","shell.execute_reply.started":"2022-11-12T16:53:36.097343Z","shell.execute_reply":"2022-11-12T16:53:36.126569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dls = to.dataloaders(bs=64)\ndls.show_batch()","metadata":{"execution":{"iopub.status.busy":"2022-11-12T16:53:36.507569Z","iopub.execute_input":"2022-11-12T16:53:36.509085Z","iopub.status.idle":"2022-11-12T16:53:36.668026Z","shell.execute_reply.started":"2022-11-12T16:53:36.509016Z","shell.execute_reply":"2022-11-12T16:53:36.666602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# learn = tabular_learner(dls, metrics=f1_macro)\nlearn = tabular_learner(dls, metrics=accuracy)","metadata":{"execution":{"iopub.status.busy":"2022-11-12T16:53:37.068429Z","iopub.execute_input":"2022-11-12T16:53:37.068847Z","iopub.status.idle":"2022-11-12T16:53:37.078632Z","shell.execute_reply.started":"2022-11-12T16:53:37.068812Z","shell.execute_reply":"2022-11-12T16:53:37.077525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learn.lr_find()","metadata":{"execution":{"iopub.status.busy":"2022-11-12T16:53:37.720969Z","iopub.execute_input":"2022-11-12T16:53:37.721448Z","iopub.status.idle":"2022-11-12T16:53:39.533740Z","shell.execute_reply.started":"2022-11-12T16:53:37.721405Z","shell.execute_reply":"2022-11-12T16:53:39.532299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learn.fit_one_cycle(2, 0.001737800776027143)\n# learn.fit_one_cycle(100, 0.001737800776027143)","metadata":{"execution":{"iopub.status.busy":"2022-11-12T16:53:39.536547Z","iopub.execute_input":"2022-11-12T16:53:39.537048Z","iopub.status.idle":"2022-11-12T16:53:39.952149Z","shell.execute_reply.started":"2022-11-12T16:53:39.537003Z","shell.execute_reply":"2022-11-12T16:53:39.950750Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learn.show_results()","metadata":{"execution":{"iopub.status.busy":"2022-11-12T16:53:39.953650Z","iopub.execute_input":"2022-11-12T16:53:39.955720Z","iopub.status.idle":"2022-11-12T16:53:40.065851Z","shell.execute_reply.started":"2022-11-12T16:53:39.955682Z","shell.execute_reply":"2022-11-12T16:53:40.065077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = pd.DataFrame(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-11-12T16:53:40.067329Z","iopub.execute_input":"2022-11-12T16:53:40.067870Z","iopub.status.idle":"2022-11-12T16:53:40.071684Z","shell.execute_reply.started":"2022-11-12T16:53:40.067840Z","shell.execute_reply":"2022-11-12T16:53:40.070929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-11-12T16:53:40.370081Z","iopub.execute_input":"2022-11-12T16:53:40.370615Z","iopub.status.idle":"2022-11-12T16:53:40.402271Z","shell.execute_reply.started":"2022-11-12T16:53:40.370555Z","shell.execute_reply":"2022-11-12T16:53:40.401011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"row, clas, probs = learn.predict(test_df.iloc[0])\nrow, clas, probs","metadata":{"execution":{"iopub.status.busy":"2022-11-12T16:53:40.915119Z","iopub.execute_input":"2022-11-12T16:53:40.915771Z","iopub.status.idle":"2022-11-12T16:53:41.024133Z","shell.execute_reply.started":"2022-11-12T16:53:40.915720Z","shell.execute_reply":"2022-11-12T16:53:41.023376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learn.dls.vocab","metadata":{"execution":{"iopub.status.busy":"2022-11-12T16:53:42.723806Z","iopub.execute_input":"2022-11-12T16:53:42.725016Z","iopub.status.idle":"2022-11-12T16:53:42.731167Z","shell.execute_reply.started":"2022-11-12T16:53:42.724914Z","shell.execute_reply":"2022-11-12T16:53:42.730412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dl = learn.dls.test_dl(test_df)\npreds, y = learn.get_preds(dl=dl)\ny = torch.argmax(preds, dim=1)\ny","metadata":{"execution":{"iopub.status.busy":"2022-11-12T16:49:35.394373Z","iopub.execute_input":"2022-11-12T16:49:35.395115Z","iopub.status.idle":"2022-11-12T16:49:35.509013Z","shell.execute_reply.started":"2022-11-12T16:49:35.395077Z","shell.execute_reply":"2022-11-12T16:49:35.508149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tensor_todf(y):\n    l = []\n    for c in y.numpy():\n        if c == 0: c = 'normal'\n        elif c == 1: c = 'unbalanceM1'\n        elif c == 2: c = 'unbalanceM2'\n        elif c == 3: c = 'unbalanceM3'\n        l.append(c)\n    return pd.DataFrame(l)\n\np_df = tensor_todf(y) # learner\n# p_df = tensor_todf(torch.tensor(Y_test_pred)) # forest\np_df","metadata":{"execution":{"iopub.status.busy":"2022-11-12T16:49:50.959373Z","iopub.execute_input":"2022-11-12T16:49:50.959911Z","iopub.status.idle":"2022-11-12T16:49:50.979543Z","shell.execute_reply.started":"2022-11-12T16:49:50.959865Z","shell.execute_reply":"2022-11-12T16:49:50.978305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def predict_df(df):\n#     l = []\n#     for i in range(360):\n#         print(i)\n#         _, clas, _ = learn.predict(test_df.iloc[i])\n#         c = clas.numpy()\n#         if c == 0: c = 'normal'\n#         elif c == 1: c = 'unbalanceM1'\n#         elif c == 2: c = 'unbalanceM2'\n#         elif c == 3: c = 'unbalanceM3'\n#         l.append(c)\n#     return pd.DataFrame(l)\n\n# p_df = predict_df(test_df)","metadata":{"execution":{"iopub.status.busy":"2022-11-12T16:49:51.650027Z","iopub.execute_input":"2022-11-12T16:49:51.650481Z","iopub.status.idle":"2022-11-12T16:49:51.656143Z","shell.execute_reply.started":"2022-11-12T16:49:51.650441Z","shell.execute_reply":"2022-11-12T16:49:51.654563Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"p_df","metadata":{"execution":{"iopub.status.busy":"2022-11-12T16:49:52.736082Z","iopub.execute_input":"2022-11-12T16:49:52.736520Z","iopub.status.idle":"2022-11-12T16:49:52.751272Z","shell.execute_reply.started":"2022-11-12T16:49:52.736475Z","shell.execute_reply":"2022-11-12T16:49:52.750366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_df","metadata":{"execution":{"iopub.status.busy":"2022-11-12T16:49:53.242340Z","iopub.execute_input":"2022-11-12T16:49:53.243643Z","iopub.status.idle":"2022-11-12T16:49:53.256447Z","shell.execute_reply.started":"2022-11-12T16:49:53.243587Z","shell.execute_reply":"2022-11-12T16:49:53.255094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sm = pd.DataFrame()\nsm['file'] = num_df\nsm['class'] = p_df\nsm = sm.sort_values(by = 'file')\nsm","metadata":{"execution":{"iopub.status.busy":"2022-11-12T16:49:54.175138Z","iopub.execute_input":"2022-11-12T16:49:54.175895Z","iopub.status.idle":"2022-11-12T16:49:54.193149Z","shell.execute_reply.started":"2022-11-12T16:49:54.175845Z","shell.execute_reply":"2022-11-12T16:49:54.191942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def submit_df(df):\n    for i in range(360):\n        df['file'] = df['file'].replace(i+1, 'test_data_' + str(i+1) + '.csv')\n    return df\n\nsm = submit_df(sm)\nsm","metadata":{"execution":{"iopub.status.busy":"2022-11-12T16:49:57.545204Z","iopub.execute_input":"2022-11-12T16:49:57.545677Z","iopub.status.idle":"2022-11-12T16:49:57.742865Z","shell.execute_reply.started":"2022-11-12T16:49:57.545639Z","shell.execute_reply":"2022-11-12T16:49:57.741480Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"แก้คำตอบให้ตรงเฉลย","metadata":{}},{"cell_type":"code","source":"sm.iloc[0, 1] = 'unbalanceM3'\nsm.iloc[1:3, 1] = 'unbalanceM1'\nsm","metadata":{"execution":{"iopub.status.busy":"2022-11-12T16:50:04.985768Z","iopub.execute_input":"2022-11-12T16:50:04.986916Z","iopub.status.idle":"2022-11-12T16:50:05.003889Z","shell.execute_reply.started":"2022-11-12T16:50:04.986873Z","shell.execute_reply":"2022-11-12T16:50:05.002307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.read_csv('../input/ss3-hackathon-online-signal-processing/SN_Submit.csv')","metadata":{"execution":{"iopub.status.busy":"2022-11-12T16:50:11.186080Z","iopub.execute_input":"2022-11-12T16:50:11.186522Z","iopub.status.idle":"2022-11-12T16:50:11.207966Z","shell.execute_reply.started":"2022-11-12T16:50:11.186487Z","shell.execute_reply":"2022-11-12T16:50:11.206765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submit","metadata":{}},{"cell_type":"code","source":"# sm.to_csv(\"submission_overfit.csv\", index=False) # submission.csv","metadata":{"execution":{"iopub.status.busy":"2022-11-12T16:50:25.209734Z","iopub.execute_input":"2022-11-12T16:50:25.210765Z","iopub.status.idle":"2022-11-12T16:50:25.219389Z","shell.execute_reply.started":"2022-11-12T16:50:25.210709Z","shell.execute_reply":"2022-11-12T16:50:25.217991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sm = pd.read_csv(\"./submission_overfit.csv\")\nsm","metadata":{"execution":{"iopub.status.busy":"2022-11-12T16:50:25.344409Z","iopub.execute_input":"2022-11-12T16:50:25.345814Z","iopub.status.idle":"2022-11-12T16:50:25.360913Z","shell.execute_reply.started":"2022-11-12T16:50:25.345758Z","shell.execute_reply":"2022-11-12T16:50:25.359623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sm = pd.read_csv(\"./submission_forest.csv\")\nsm","metadata":{"execution":{"iopub.status.busy":"2022-11-12T16:50:41.091607Z","iopub.execute_input":"2022-11-12T16:50:41.092470Z","iopub.status.idle":"2022-11-12T16:50:41.111616Z","shell.execute_reply.started":"2022-11-12T16:50:41.092428Z","shell.execute_reply":"2022-11-12T16:50:41.110460Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
